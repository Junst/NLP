{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 챗봇 문답 데이터 감정 분류 모델 구현"
      ],
      "metadata": {
        "id": "ik4ZOiI0r6UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "예제로 제공되는 데이터는 [github.com/songys/Chatbot_data](https://github.com/songys/Chatbot_data)에서 공개하신 한국어 챗봇 데이터입니다."
      ],
      "metadata": {
        "id": "FonAhtKNsBjB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eNFYv9JRr05p"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽어오기\n",
        "train_file = \"ChatbotData.csv\"\n",
        "data = pd.read_csv(train_file, delimiter= ',')\n",
        "features = data['Q'].tolist()\n",
        "labels = data['label'].tolist()"
      ],
      "metadata": {
        "id": "2ZmAw-yysoow"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 인덱스 시퀀스 벡터\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features] # 위에서 불러온 질문 리스트(features)에서 문장을 하나씩 꺼내와 text_toword_sequence() 함수를 이용해 단어 시퀀스를 만든다.\n",
        "# 단어 시퀀스란 단어 토큰들의 순차적 리스트를 의미한다. 예를 들어 '아라시는 결성한지 23주년입니다.'는 ['아라시는', '결성한지', '23주년입니다']가 된다.\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus) # 생성된 단어 시퀀스를 말뭉치(corpus) 리스트에 저장한다.\n",
        "sequences = tokenizer.texts_to_sequences(corpus) # texts_to_sequences() 함수를 이용해 문장 내 모든 단어를 시퀀스 번호로 변환한다. 이를 통해 단어 임베딩 벡터를 만든다.\n",
        "word_index = tokenizer.word_index"
      ],
      "metadata": {
        "id": "SG5YXsAAszpk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**시퀀스 번호로 만든 벡터는 한 가지 문제가 있는데, 바로 문장의 길이가 제각각이기 때문에 벡터 크기가 모두 다르다. 따라서 시퀀스 번호로 변환된 전체 벡터 크기를 동일하게 맞춰야 한다. MAX_SEQ_LEN 크기만큼 늘리고, MAX_SEQ_LEN보다 작은 벡터는 남는 공간이므로 0으로 채우는 작업을 한다. 이를 패딩(Padding)이라고 한다.**"
      ],
      "metadata": {
        "id": "2lcMaojgu4-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post') # pad_sequences () 함수를 통해 시퀀스의 패딩 처리를 할 수 있다."
      ],
      "metadata": {
        "id": "gFBIU99itHeP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습용, 검증용, 테스트용 데이터셋 생성\n",
        "# 학습셋 : 검증셋 : 테스트셋 = 7:2:1\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds= ds.shuffle(len(features))\n",
        "\n",
        "train_size = int(len(padded_seqs)*0.7)\n",
        "val_size = int(len(padded_seqs)*0.2)\n",
        "test_size = int(len(padded_seqs)*0.1)\n",
        "\n",
        "train_ds = ds.take(train_size).batch(20)\n",
        "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
        "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)"
      ],
      "metadata": {
        "id": "wGazrMa1tZP7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "dropout_prob = 0.5\n",
        "EMB_SIZE = 128\n",
        "EPOCH = 5\n",
        "VOCAB_SIZE = len(word_index) + 1 # 전체 단어 수"
      ],
      "metadata": {
        "id": "x1wB7EIivuwk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN 모델을 케라스 함수형 모델(functional model) 방식으로 구현했다.\n",
        "\n",
        "문장을 감정 클래스로 분류하는 CNN 모델은 전처리된 입력 데이터를 단어 임베딩 처리하는 영역과 합성곱 필터와 연산을 통해 문장의 특징 정보(특징맵)를 추출하고, 평탄화(flatten)를 하는 영역, 그리고 완전 연결 계층(fully connected layer)을 통해 감정별로 클래스를 분류하는 영역으로 구성된다."
      ],
      "metadata": {
        "id": "CyGz0CGK3q0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 모델 정의\n",
        "\n",
        "# 입력 계층은 keras의 Input()으로 생성, shape의 인자로 입력 노드에 들어올 데이터의 형상(shape)을 지정한다.\n",
        "# 실제 패딩처리된 시퀀스 벡터의 크기(MAX_SEQ_LEN)로 설정한다.\n",
        "input_layer = Input(shape=(MAX_SEQ_LEN,)) \n",
        "\n",
        "# 앞서 단어별로 패딩 처리된 시퀀스 벡터는 희소벡터다.\n",
        "# 임베딩 계층은 희소벡터를 입력받아 데이터 손실을 최소화하면서 벡터 차원이 압축되는 밀집 벡터로 변환해준다.\n",
        "# 단어의 개수(VOCAB_SIZE)와 임베딩 결과로 나올 밀집 벡터의 크기(EMB_SIZE), 입력되는 시퀀스 벡터의 크기(MAX_SEQ_LEN)를 Embedding()의 인자로 사용해 임베딩 계층을 생성한다.\n",
        "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length = MAX_SEQ_LEN)(input_layer) \n",
        "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer) # Overfitting(과적합)을 막기 위해 50% 확률로 Dropout()\n",
        "\n",
        "# 임베딩 계층을 통해 전달된 임베딩 벡터에서 특징 추출을 하는 영역 구현\n",
        "# Conv1D()을 이용해 크기가 3, 4, 5인 합성곱 filter를 128개씩 사용한 합성곱 계층을 3개 생성, 이는 3, 4, 5-gram 언어 모델의 개념고 ㅏ비슷하다.\n",
        "conv1=Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=3,\n",
        "    padding='valid',\n",
        "    activation = tf.nn.relu)(dropout_emb)\n",
        "pool1 =GlobalMaxPool1D()(conv1) # 임베딩 벡터를 합성곱 계층의 입력으로 받아 GlobalMaxPool1D()를 이용해 최대 풀링 연산을 수행한다.\n",
        "\n",
        "conv2 = Conv1D(\n",
        "    filters = 128,\n",
        "    kernel_size=4,\n",
        "    padding='valid',\n",
        "    activation= tf.nn.relu)(dropout_emb)\n",
        "pool2= GlobalMaxPool1D()(conv2)\n",
        "\n",
        "conv3 = Conv1D(\n",
        "    filters=128,\n",
        "    kernel_size=5,\n",
        "    padding='valid',\n",
        "    activation = tf.nn.relu)(dropout_emb)\n",
        "pool3 = GlobalMaxPool1D()(conv3)"
      ],
      "metadata": {
        "id": "DURnSxY7v9J9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3, 4, 5-gram 이후 합치기\n",
        "concat = concatenate([pool1,pool2,pool3]) #완전 연결 계층에 전달될 수 있도록 concatenate()를 이용해 각각 병렬로 처리된 합성곱 계층의 특징맵 결과를 하나로 묶어준다.\n",
        "\n",
        "# Dense()를 이용해 128개의 출력노드를 가지고, relu 활성화 함수를 사용하는 Dense 계층을 생성한다.\n",
        "# 이 Dense 계층은 이전 계층에서 합성곱 연산과 맥스 풀링으로 나온 3개의 특징맵 데이터를 입력으로 받는다.\n",
        "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
        "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
        "\n",
        "# 챗봇 데이터 문장에서 3가지 클래스로 감정 분류해야 하기 때문에 출력노드가 3개인 Dense()를 생성한다.\n",
        "# 이 때 최종 단계이기 때문에 활성화 함수를 사용하지 않는다.\n",
        "# 이번 계층에서 결과로 나온 값을 logits, score(점수)라고 부른다.\n",
        "logits= Dense(3, name='logits')(dropout_hidden)\n",
        "predictions = Dense(3, activation= tf.nn.softmax)(logits)"
      ],
      "metadata": {
        "id": "WSVA-_JJzUuq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model= Model(inputs=input_layer, outputs=predictions)\n",
        "model.compile(optimizer='adam',\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "MIm5oNXN1T0a"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqlkZVuQ1hgr",
        "outputId": "e3d1bdfe-e162-4f78-9996-126162e632b5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "414/414 [==============================] - 16s 35ms/step - loss: 0.8960 - accuracy: 0.5632 - val_loss: 0.5428 - val_accuracy: 0.7944\n",
            "Epoch 2/5\n",
            "414/414 [==============================] - 13s 32ms/step - loss: 0.5257 - accuracy: 0.8001 - val_loss: 0.2693 - val_accuracy: 0.9192\n",
            "Epoch 3/5\n",
            "414/414 [==============================] - 13s 32ms/step - loss: 0.3125 - accuracy: 0.8931 - val_loss: 0.1606 - val_accuracy: 0.9509\n",
            "Epoch 4/5\n",
            "414/414 [==============================] - 13s 32ms/step - loss: 0.1993 - accuracy: 0.9386 - val_loss: 0.1062 - val_accuracy: 0.9700\n",
            "Epoch 5/5\n",
            "414/414 [==============================] - 13s 32ms/step - loss: 0.1393 - accuracy: 0.9578 - val_loss: 0.0617 - val_accuracy: 0.9805\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbbee305050>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가(테스트 데이터셋 이용)\n",
        "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
        "print('Accuracy: %f' % (accuracy*100))\n",
        "print('loss: %f' % (loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-NE-Hiu1ngb",
        "outputId": "089f5968-1b53-4267-9a85-e9f591ce4106"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60/60 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9831\n",
            "Accuracy: 98.307955\n",
            "loss: 0.054410\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "model.save('cnn_model.h5')"
      ],
      "metadata": {
        "id": "o9KDDC9y1387"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#챗봇 문답 데이터 감정 분류 모델 사용"
      ],
      "metadata": {
        "id": "gRGToZOM5_vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras import preprocessing"
      ],
      "metadata": {
        "id": "HEKTF8F55_XR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 읽어오기\n",
        "train_file = \"ChatbotData.csv\"\n",
        "data = pd.read_csv(train_file, delimiter=',') # read_csv() 함수를 이용해 파일을 읽어와 \n",
        "features= data['Q'].tolist() # label(감정)을 분류할 Q (질문) 데이터를 features 리스트에 저장한다.\n",
        "labels = data['label'].tolist() # labels 리스트는 CNN 모델이 예측한 분류 결과와 실제 분류값을 비교하기 위한 목적"
      ],
      "metadata": {
        "id": "g3cr4HMJ6NAp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 인덱스 시퀀스 벡터\n",
        "\n",
        "# 위에서 불러온 질문 리스트(features)에서 한 문장씩 꺼내와 text_to_word_sequence() 함수를 이용해 단어 시퀀스를 만든 후 말뭉치(corpus) 리스트에 저장한다.\n",
        "corpus = [preprocessing.text.text_to_word_sequence(text) for text in features]\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "sequences= tokenizer.texts_to_sequences(corpus) # 모든 단어를 시퀀스 번호로 변환한다."
      ],
      "metadata": {
        "id": "czFeW3nM6ewd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 15 # 단어 시퀀스 벡터 크기\n",
        "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen = MAX_SEQ_LEN, padding='post') # pad_ sequences() 함수를 통해 단어 시퀀스 벡터 크기를 맞춰 패딩 처리를 한다."
      ],
      "metadata": {
        "id": "gG-x3N826t9u"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 데이터셋 생성\n",
        "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, labels))\n",
        "ds = ds.shuffle(len(features))\n",
        "test_ds=ds.take(2000).batch(20)"
      ],
      "metadata": {
        "id": "57bIM8_L68EU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 감정 분류 CNN 모델 불러오기\n",
        "model = load_model('cnn_model.h5') # load_model() 함수르ㅡㄹ 이용해 모델 파일을 불러온다.\n",
        "model.summary() # summary() 함수를 호출하고, 테스트셋 데이털르 이용해 모델 성능을 평가한다.\n",
        "model.evaluate(test_ds, verbose=2) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjUUtcyg7HWg",
        "outputId": "e2a1e862-e469-44ad-f946-30647a0a5cf4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)           [(None, 15)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 15, 128)      1715072     ['input_5[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 15, 128)      0           ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 13, 128)      49280       ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 12, 128)      65664       ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 11, 128)      82048       ['dropout_4[0][0]']              \n",
            "                                                                                                  \n",
            " global_max_pooling1d_3 (Global  (None, 128)         0           ['conv1d_4[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_4 (Global  (None, 128)         0           ['conv1d_5[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " global_max_pooling1d_5 (Global  (None, 128)         0           ['conv1d_6[0][0]']               \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 384)          0           ['global_max_pooling1d_3[0][0]', \n",
            "                                                                  'global_max_pooling1d_4[0][0]', \n",
            "                                                                  'global_max_pooling1d_5[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          49280       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " logits (Dense)                 (None, 3)            387         ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 3)            12          ['logits[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,961,743\n",
            "Trainable params: 1,961,743\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "100/100 - 1s - loss: 0.0595 - accuracy: 0.9830 - 658ms/epoch - 7ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.059533897787332535, 0.9829999804496765]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트용 데이터셋의 10212번째 데이터 출력\n",
        "print(\"단어 시퀀스 : \", corpus[10212])\n",
        "print(\"단어 인덱스 시퀀스 : \", padded_seqs[10212])\n",
        "print(\"문장 분류(정답) : \", labels[10212]) # 2는 \"사랑(Label:2)\"이다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuLQ6sFF7QFG",
        "outputId": "42209e90-be06-4d97-b91b-4525392dc153"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 시퀀스 :  ['썸', '타는', '여자가', '남사친', '만나러', '간다는데', '뭐라', '해']\n",
            "단어 인덱스 시퀀스 :  [   13    61   127  4320  1333 12162   856    31     0     0     0     0\n",
            "     0     0     0]\n",
            "문장 분류(정답) :  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#테스트용 데이터셋의 10212번째 데이터 감정 예측\n",
        "picks = [10212]\n",
        "predict = model.predict(padded_seqs[picks]) # predict() 함수는 입력 데이터에 대해 각 클래스별로 예측한 점수를 반환한다.\n",
        "predict_class = tf.math.argmax(predict,axis=1) #argmax() 함수를 이용해 분류 클래스들 중 예측 점수가 가장 큰 클래스 번호를 계산한다. 즉, 10212번째 문장이 어떤 감정 클래스에 포함되어 있는지 판단한다.\n",
        "print(\"감정 예측 점수 : \", predict)\n",
        "print(\"감정 예측 클래스 : \", predict_class.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYdlbRov7cHV",
        "outputId": "9631e661-fb4f-47b5-c15e-ba0b13c44e2d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "감정 예측 점수 :  [[1.1718092e-06 9.2785473e-07 9.9999785e-01]]\n",
            "감정 예측 클래스 :  [2]\n"
          ]
        }
      ]
    }
  ]
}